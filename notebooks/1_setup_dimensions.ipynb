{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a998adeb-ca70-4be0-93ab-534e09290ee7",
   "metadata": {},
   "source": [
    "### Import Libraries and Initialize PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "635c4596-79ca-422c-96af-bf12a47dc95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "\n",
    "print(\"✓ Libraries imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af2e9bf-228f-4d66-8ee0-ef138f450f04",
   "metadata": {},
   "source": [
    "### Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5db3e4f3-de0d-426c-b263-520f0ef6e584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Spark Session created\n",
      "  Version: 4.1.0\n",
      "  App Name: Project2 - Fashion Retail Streaming Data Lakehouse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/17 01:23:43 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# Create Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Project2 - Fashion Retail Streaming Data Lakehouse\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", os.path.abspath(\"../lakehouse\")) \\\n",
    "    .config(\"spark.sql.streaming.schemaInference\", \"true\") \\\n",
    "    .config(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(f\"✓ Spark Session created\")\n",
    "print(f\"  Version: {spark.version}\")\n",
    "print(f\"  App Name: {spark.sparkContext.appName}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7131836-64f5-473c-a212-54ff66d49684",
   "metadata": {},
   "source": [
    "### Load Customer Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15c2e5dc-6df9-49c3-a682-d7b7e49bbd48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual columns in CSV:\n",
      "['customer_reference_id', 'first_name', 'last_name', 'email', 'phone', 'city', 'state', 'zip_code', 'registration_date', 'customer_segment', 'loyalty_tier', 'age']\n",
      "\n",
      "✓ Customer dimension loaded: 166 records\n",
      "+------------+---------------------+----------+---------+--------------------+------------+---+------------+--------------+\n",
      "|customer_key|customer_reference_id|first_name|last_name|               email|        city|age|loyalty_tier|loyalty_member|\n",
      "+------------+---------------------+----------+---------+--------------------+------------+---+------------+--------------+\n",
      "|           1|                 3957|     Aiden|    Davis|aiden.davis26@ema...|   San Diego| 20|    Platinum|           Yes|\n",
      "|           2|                 3958|    Olivia|   Garcia|olivia.garcia224@...|     Seattle| 46|      Silver|            No|\n",
      "|           3|                 3959|     Grace|    Moore|grace.moore829@em...|   Las Vegas| 66|      Silver|            No|\n",
      "|           4|                 3960|   Michael|   Miller|michael.miller95@...|Jacksonville| 42|      Bronze|            No|\n",
      "|           5|                 3961|       Mia|    Scott|mia.scott301@emai...|      Denver| 23|        Gold|           Yes|\n",
      "+------------+---------------------+----------+---------+--------------------+------------+---+------------+--------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/17 01:29:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/17 01:29:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/17 01:29:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "# Load customers from CSV\n",
    "df_dim_customers = spark.read.csv(\"../data/customers.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# First, let's see what columns we actually have\n",
    "print(\"Actual columns in CSV:\")\n",
    "print(df_dim_customers.columns)\n",
    "print()\n",
    "\n",
    "# Add customer_key (surrogate key) - use lowercase column name\n",
    "from pyspark.sql.window import Window\n",
    "window_spec = Window.orderBy(\"customer_reference_id\")\n",
    "df_dim_customers = df_dim_customers.withColumn(\"customer_key\", row_number().over(window_spec))\n",
    "\n",
    "# Select columns that exist (using lowercase names from your CSV)\n",
    "df_dim_customers = df_dim_customers.select(\n",
    "    col(\"customer_key\"),\n",
    "    col(\"customer_reference_id\"),\n",
    "    col(\"first_name\"),\n",
    "    col(\"last_name\"),\n",
    "    col(\"email\"),\n",
    "    col(\"city\"),\n",
    "    col(\"age\"),\n",
    "    col(\"loyalty_tier\")\n",
    ")\n",
    "\n",
    "# Derive loyalty_member (Yes/No)\n",
    "df_dim_customers = df_dim_customers.withColumn(\n",
    "    \"loyalty_member\",\n",
    "    when(col(\"loyalty_tier\").isin(\"Gold\", \"Platinum\"), \"Yes\").otherwise(\"No\")\n",
    ")\n",
    "\n",
    "print(f\"✓ Customer dimension loaded: {df_dim_customers.count()} records\")\n",
    "df_dim_customers.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18173f7b-d9da-4b0b-bbc9-143bd76b1bab",
   "metadata": {},
   "source": [
    "### Load Product Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a92484a9-8d5b-4f6e-a6e0-42769c925085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual columns in CSV:\n",
      "['product_id', 'item_name', 'category', 'brand', 'material', 'season', 'gender_target', 'base_price', 'stock_quantity', 'supplier_name', 'product_introduction_date']\n",
      "\n",
      "✓ Product dimension loaded: 61 records\n",
      "+-----------+---------+-----------+---------------+---------+-------------+-------------+----------+--------------+\n",
      "|product_key|item_name|   category|          brand| material|       season|gender_target|base_price|stock_quantity|\n",
      "+-----------+---------+-----------+---------------+---------+-------------+-------------+----------+--------------+\n",
      "|          1| Backpack|Accessories|Premium Fashion|Synthetic|Spring/Summer|          Men|      4657|           120|\n",
      "|          2|     Belt|Accessories|Premium Fashion|    Suede|  Fall/Winter|        Women|      4841|            58|\n",
      "|          3|   Blazer|     Formal|  Fashion House|     Wool|  Fall/Winter|       Unisex|      3848|           396|\n",
      "|          4|   Blouse|       Tops| Boutique Brand|Polyester|Spring/Summer|       Unisex|      3113|           241|\n",
      "|          5|    Boots|   Footwear|     Urban Chic|  Leather|   All Season|        Women|      4793|           292|\n",
      "+-----------+---------+-----------+---------------+---------+-------------+-------------+----------+--------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/17 01:31:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/17 01:31:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/17 01:31:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "# Load products from CSV\n",
    "df_dim_products = spark.read.csv(\"../data/products.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# First, let's see what columns we actually have\n",
    "print(\"Actual columns in CSV:\")\n",
    "print(df_dim_products.columns)\n",
    "print()\n",
    "\n",
    "# Add product_key (surrogate key) - use lowercase column name\n",
    "window_spec = Window.orderBy(\"item_name\")\n",
    "df_dim_products = df_dim_products.withColumn(\"product_key\", row_number().over(window_spec))\n",
    "\n",
    "# Select columns that exist (using lowercase names from your CSV)\n",
    "df_dim_products = df_dim_products.select(\n",
    "    col(\"product_key\"),\n",
    "    col(\"item_name\"),\n",
    "    col(\"category\"),\n",
    "    col(\"brand\"),\n",
    "    col(\"material\"),\n",
    "    col(\"season\"),\n",
    "    col(\"gender_target\"),\n",
    "    col(\"base_price\"),\n",
    "    col(\"stock_quantity\")\n",
    ")\n",
    "\n",
    "print(f\"✓ Product dimension loaded: {df_dim_products.count()} records\")\n",
    "df_dim_products.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d961e254-5325-4982-bf2e-e9b2d9bf873e",
   "metadata": {},
   "source": [
    "### Generate Date Dimension "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b3d1ad2-878d-4af9-931a-f3cc72b1d563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Date dimension generated: 365 records\n",
      "+--------+---+---------+-------------------+-----+----------+-------------+-------+----+\n",
      "|date_key|day| day_name|          full_date|month|month_name|purchase_date|quarter|year|\n",
      "+--------+---+---------+-------------------+-----+----------+-------------+-------+----+\n",
      "|       1|  1|   Sunday|2023-01-01 00:00:00|    1|   January|   2023-01-01|      1|2023|\n",
      "|       2|  2|   Monday|2023-01-02 00:00:00|    1|   January|   2023-01-02|      1|2023|\n",
      "|       3|  3|  Tuesday|2023-01-03 00:00:00|    1|   January|   2023-01-03|      1|2023|\n",
      "|       4|  4|Wednesday|2023-01-04 00:00:00|    1|   January|   2023-01-04|      1|2023|\n",
      "|       5|  5| Thursday|2023-01-05 00:00:00|    1|   January|   2023-01-05|      1|2023|\n",
      "+--------+---+---------+-------------------+-----+----------+-------------+-------+----+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Generate date dimension covering sales period (2023)\n",
    "dates = []\n",
    "start_date = datetime(2023, 1, 1)\n",
    "\n",
    "for i in range(365):\n",
    "    date = start_date + timedelta(days=i)\n",
    "    dates.append({\n",
    "        'date_key': i + 1,\n",
    "        'purchase_date': date.strftime('%Y-%m-%d'),\n",
    "        'full_date': date,\n",
    "        'year': date.year,\n",
    "        'month': date.month,\n",
    "        'day': date.day,\n",
    "        'quarter': (date.month - 1) // 3 + 1,\n",
    "        'month_name': date.strftime('%B'),\n",
    "        'day_name': date.strftime('%A')\n",
    "    })\n",
    "\n",
    "df_dim_date = spark.createDataFrame(dates)\n",
    "\n",
    "print(f\"✓ Date dimension generated: {df_dim_date.count()} records\")\n",
    "df_dim_date.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb354b48-2dff-4ee9-a881-163b15fea9c9",
   "metadata": {},
   "source": [
    "### Create Payment Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bdb1fead-83e3-4d86-ac78-904464f1b9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Payment dimension created: 4 records\n",
      "+-----------+--------------+\n",
      "|payment_key|payment_method|\n",
      "+-----------+--------------+\n",
      "|          1|   Credit Card|\n",
      "|          2|        PayPal|\n",
      "|          3|          Cash|\n",
      "|          4|    Debit Card|\n",
      "+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create payment dimension (simple lookup table)\n",
    "payment_data = [\n",
    "    {'payment_key': 1, 'payment_method': 'Credit Card'},\n",
    "    {'payment_key': 2, 'payment_method': 'PayPal'},\n",
    "    {'payment_key': 3, 'payment_method': 'Cash'},\n",
    "    {'payment_key': 4, 'payment_method': 'Debit Card'}\n",
    "]\n",
    "\n",
    "df_dim_payment = spark.createDataFrame(payment_data)\n",
    "\n",
    "print(f\"✓ Payment dimension created: {df_dim_payment.count()} records\")\n",
    "df_dim_payment.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a12646-26ab-4620-9a91-68520114a72c",
   "metadata": {},
   "source": [
    "### Summary of All Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "922580d9-a403-4d8a-ac71-abf33206623a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DIMENSION TABLES SUMMARY\n",
      "============================================================\n",
      "✓ Customers: 166 records\n",
      "✓ Products: 61 records\n",
      "✓ Dates: 365 records\n",
      "✓ Payment Methods: 4 records\n",
      "============================================================\n",
      "✅ All dimensions ready for streaming ETL!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"DIMENSION TABLES SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"✓ Customers: {df_dim_customers.count()} records\")\n",
    "print(f\"✓ Products: {df_dim_products.count()} records\")\n",
    "print(f\"✓ Dates: {df_dim_date.count()} records\")\n",
    "print(f\"✓ Payment Methods: {df_dim_payment.count()} records\")\n",
    "print(\"=\"*60)\n",
    "print(\"✅ All dimensions ready for streaming ETL!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c560d560-f43e-493d-b8c4-754d4db7ec01",
   "metadata": {},
   "source": [
    "### Verify Column Names for Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9310610f-ccb5-46b1-a557-a24da86a0edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer columns: ['customer_key', 'customer_reference_id', 'first_name', 'last_name', 'email', 'city', 'age', 'loyalty_tier', 'loyalty_member']\n",
      "Product columns: ['product_key', 'item_name', 'category', 'brand', 'material', 'season', 'gender_target', 'base_price', 'stock_quantity']\n",
      "Date columns: ['date_key', 'day', 'day_name', 'full_date', 'month', 'month_name', 'purchase_date', 'quarter', 'year']\n",
      "Payment columns: ['payment_key', 'payment_method']\n"
     ]
    }
   ],
   "source": [
    "print(\"Customer columns:\", df_dim_customers.columns)\n",
    "print(\"Product columns:\", df_dim_products.columns)\n",
    "print(\"Date columns:\", df_dim_date.columns)\n",
    "print(\"Payment columns:\", df_dim_payment.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
