{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "180c2c12-71cc-43af-a20f-f2220d9673e0",
   "metadata": {},
   "source": [
    "### Import Libraries and Load Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c17eca9-241d-4ad0-942a-48dbc6a30647",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/18 05:00:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/12/18 05:00:33 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.\n",
      "java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension\n",
      "\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n",
      "\tat java.base/java.lang.Class.forName0(Native Method)\n",
      "\tat java.base/java.lang.Class.forName(Class.java:529)\n",
      "\tat java.base/java.lang.Class.forName(Class.java:508)\n",
      "\tat org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:42)\n",
      "\tat org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:37)\n",
      "\tat org.apache.spark.util.Utils$.classForName(Utils.scala:97)\n",
      "\tat org.apache.spark.sql.classic.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1232)\n",
      "\tat org.apache.spark.sql.classic.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1230)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:630)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:628)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:936)\n",
      "\tat org.apache.spark.sql.classic.SparkSession$.org$apache$spark$sql$classic$SparkSession$$applyExtensions(SparkSession.scala:1230)\n",
      "\tat org.apache.spark.sql.classic.SparkSession$.applyAndLoadExtensions(SparkSession.scala:1214)\n",
      "\tat org.apache.spark.sql.classic.SparkSession.<init>(SparkSession.scala:117)\n",
      "\tat java.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:62)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Spark Session created for streaming ETL\n",
      "  Version: 4.1.0\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "import os\n",
    "\n",
    "# Create Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Project2 - Streaming ETL Pipeline\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", os.path.abspath(\"../lakehouse\")) \\\n",
    "    .config(\"spark.sql.streaming.schemaInference\", \"true\") \\\n",
    "    .config(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\"âœ“ Spark Session created for streaming ETL\")\n",
    "print(f\"  Version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da1960f-afa3-44ce-b64b-a241c992bc03",
   "metadata": {},
   "source": [
    "### Load All Dimension Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4443431a-64fb-4737-b6fe-3b8f29c3d287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LOADING DIMENSIONS FROM MONGODB ATLAS\n",
      "============================================================\n",
      "\n",
      "ðŸ“Š Loading customers from MongoDB Atlas...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Loaded 166 customers from MongoDB Atlas\n",
      "\n",
      "ðŸ“¦ Loading products from MongoDB Atlas...\n",
      "âœ“ Loaded 61 products from MongoDB Atlas\n",
      "\n",
      "ðŸ“… Generating date dimension...\n",
      "âœ“ Generated 365 date records\n",
      "\n",
      "ðŸ’³ Creating payment dimension...\n",
      "âœ“ Created 4 payment methods\n",
      "\n",
      "============================================================\n",
      "DIMENSION TABLES LOADED\n",
      "============================================================\n",
      "âœ“ Customers (MongoDB): 166 records\n",
      "âœ“ Products (MongoDB): 61 records\n",
      "âœ“ Dates (Generated): 365 records\n",
      "âœ“ Payment Methods: 4 records\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "\n",
    "# MongoDB connection\n",
    "mongodb_uri = \"mongodb+srv://simonalam1234_db_user:DcBwszDL4I7auowi@cluster.axefz7f.mongodb.net/?appName=cluster\"\n",
    "client = pymongo.MongoClient(mongodb_uri)\n",
    "db = client[\"fashion_retail_lakehouse\"]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LOADING DIMENSIONS FROM MONGODB ATLAS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load Customers from MongoDB Atlas\n",
    "print(\"\\nðŸ“Š Loading customers from MongoDB Atlas...\")\n",
    "customers_from_mongo = list(db.customers.find())\n",
    "df_dim_customers_pandas = pd.DataFrame(customers_from_mongo)\n",
    "\n",
    "# Remove MongoDB's _id field\n",
    "if '_id' in df_dim_customers_pandas.columns:\n",
    "    df_dim_customers_pandas = df_dim_customers_pandas.drop('_id', axis=1)\n",
    "\n",
    "# Convert to Spark DataFrame\n",
    "df_dim_customers = spark.createDataFrame(df_dim_customers_pandas)\n",
    "\n",
    "# Add customer_key\n",
    "from pyspark.sql.window import Window\n",
    "window_spec = Window.orderBy(\"customer_reference_id\")\n",
    "df_dim_customers = df_dim_customers.withColumn(\"customer_key\", row_number().over(window_spec))\n",
    "\n",
    "# Select and transform\n",
    "df_dim_customers = df_dim_customers.select(\n",
    "    col(\"customer_key\"),\n",
    "    col(\"customer_reference_id\"),\n",
    "    col(\"first_name\"),\n",
    "    col(\"last_name\"),\n",
    "    col(\"email\"),\n",
    "    col(\"city\"),\n",
    "    col(\"age\"),\n",
    "    col(\"loyalty_tier\")\n",
    ").withColumn(\n",
    "    \"loyalty_member\",\n",
    "    when(col(\"loyalty_tier\").isin(\"Gold\", \"Platinum\"), \"Yes\").otherwise(\"No\")\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Loaded {df_dim_customers.count()} customers from MongoDB Atlas\")\n",
    "\n",
    "# Load Products from MongoDB Atlas\n",
    "print(\"\\nðŸ“¦ Loading products from MongoDB Atlas...\")\n",
    "products_from_mongo = list(db.products.find())\n",
    "df_dim_products_pandas = pd.DataFrame(products_from_mongo)\n",
    "\n",
    "# Remove MongoDB's _id field\n",
    "if '_id' in df_dim_products_pandas.columns:\n",
    "    df_dim_products_pandas = df_dim_products_pandas.drop('_id', axis=1)\n",
    "\n",
    "df_dim_products = spark.createDataFrame(df_dim_products_pandas)\n",
    "window_spec = Window.orderBy(\"item_name\")\n",
    "df_dim_products = df_dim_products.withColumn(\"product_key\", row_number().over(window_spec))\n",
    "\n",
    "df_dim_products = df_dim_products.select(\n",
    "    col(\"product_key\"),\n",
    "    col(\"item_name\"),\n",
    "    col(\"category\"),\n",
    "    col(\"brand\"),\n",
    "    col(\"material\"),\n",
    "    col(\"season\"),\n",
    "    col(\"gender_target\"),\n",
    "    col(\"base_price\"),\n",
    "    col(\"stock_quantity\")\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Loaded {df_dim_products.count()} products from MongoDB Atlas\")\n",
    "\n",
    "# Generate Date Dimension (programmatic - same as before)\n",
    "print(\"\\nðŸ“… Generating date dimension...\")\n",
    "from datetime import datetime, timedelta\n",
    "dates = []\n",
    "start_date = datetime(2023, 1, 1)\n",
    "for i in range(365):\n",
    "    date = start_date + timedelta(days=i)\n",
    "    dates.append({\n",
    "        'date_key': i + 1,\n",
    "        'purchase_date': date.strftime('%Y-%m-%d'),\n",
    "        'full_date': date,\n",
    "        'year': date.year,\n",
    "        'month': date.month,\n",
    "        'day': date.day,\n",
    "        'quarter': (date.month - 1) // 3 + 1,\n",
    "        'month_name': date.strftime('%B'),\n",
    "        'day_name': date.strftime('%A')\n",
    "    })\n",
    "df_dim_date = spark.createDataFrame(dates)\n",
    "\n",
    "print(f\"âœ“ Generated {df_dim_date.count()} date records\")\n",
    "\n",
    "# Create Payment Dimension (in-memory - same as before)\n",
    "print(\"\\nðŸ’³ Creating payment dimension...\")\n",
    "payment_data = [\n",
    "    {'payment_key': 1, 'payment_method': 'Credit Card'},\n",
    "    {'payment_key': 2, 'payment_method': 'PayPal'},\n",
    "    {'payment_key': 3, 'payment_method': 'Cash'},\n",
    "    {'payment_key': 4, 'payment_method': 'Debit Card'}\n",
    "]\n",
    "df_dim_payment = spark.createDataFrame(payment_data)\n",
    "\n",
    "print(f\"âœ“ Created {df_dim_payment.count()} payment methods\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DIMENSION TABLES LOADED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"âœ“ Customers (MongoDB): {df_dim_customers.count()} records\")\n",
    "print(f\"âœ“ Products (MongoDB): {df_dim_products.count()} records\")\n",
    "print(f\"âœ“ Dates (Generated): {df_dim_date.count()} records\")\n",
    "print(f\"âœ“ Payment Methods: {df_dim_payment.count()} records\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37f23fb-04e3-4deb-b3cb-3455cf2ca75f",
   "metadata": {},
   "source": [
    "### Bronze Layer - Read Streaming Sales Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94338975-d0fc-47a8-8705-3a60de23ee1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Bronze streaming query defined\n",
      "  Source: ../streaming/sales/\n",
      "  Output: ../lakehouse/sales/bronze\n",
      "  Is Streaming: True\n"
     ]
    }
   ],
   "source": [
    "# Define output paths\n",
    "bronze_output = \"../lakehouse/sales/bronze\"\n",
    "bronze_checkpoint = os.path.join(bronze_output, \"_checkpoint\")\n",
    "\n",
    "# Read streaming sales transactions from JSON files\n",
    "df_sales_bronze = (\n",
    "    spark.readStream \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .option(\"multiLine\", \"true\") \\\n",
    "    .json(\"../streaming/sales/\")\n",
    "    .withColumn(\"receipt_time\", current_timestamp())\n",
    "    .withColumn(\"source_file\", input_file_name())\n",
    ")\n",
    "\n",
    "print(\"âœ“ Bronze streaming query defined\")\n",
    "print(f\"  Source: ../streaming/sales/\")\n",
    "print(f\"  Output: {bronze_output}\")\n",
    "print(f\"  Is Streaming: {df_sales_bronze.isStreaming}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7df88d08-f1ae-4eae-b4ac-539b9304e064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Deleted: ../lakehouse/sales/bronze\n",
      "âœ“ Deleted: ../lakehouse/sales/silver\n",
      "\n",
      "âœ… Ready to reprocess ALL 10 JSON files\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# Stop all active queries\n",
    "for query in spark.streams.active:\n",
    "    try:\n",
    "        query.stop()\n",
    "        print(f\"âœ“ Stopped query: {query.name}\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Define paths\n",
    "silver_output = \"../lakehouse/sales/silver\"\n",
    "\n",
    "# DELETE bronze and silver directories completely\n",
    "for path in [bronze_output, silver_output]:\n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree(path)\n",
    "        print(f\"âœ“ Deleted: {path}\")\n",
    "\n",
    "print(\"\\nâœ… Ready to reprocess ALL 10 JSON files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c783071-f9b0-441f-bc5e-eb5cff1a2da0",
   "metadata": {},
   "source": [
    "### Write Bronze Layer to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc853e0e-1883-4125-9b45-c4f3af7d1804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Read 3400 records from JSON files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/18 05:00:40 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/12/18 05:00:40 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/12/18 05:00:40 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Bronze layer complete!\n",
      "âœ“ Total records in bronze: 3400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/18 05:00:40 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/12/18 05:00:40 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    }
   ],
   "source": [
    "# Clean up checkpoint\n",
    "import shutil\n",
    "if os.path.exists(bronze_checkpoint):\n",
    "    shutil.rmtree(bronze_checkpoint)\n",
    "    print(\"âœ“ Cleaned old checkpoint\")\n",
    "\n",
    "# Define explicit schema\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, LongType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"Customer Reference ID\", LongType(), True),\n",
    "    StructField(\"Date Purchase\", StringType(), True),\n",
    "    StructField(\"Item Purchased\", StringType(), True),\n",
    "    StructField(\"Payment Method\", StringType(), True),\n",
    "    StructField(\"Purchase Amount (USD)\", DoubleType(), True),\n",
    "    StructField(\"Review Rating\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Read JSON files (line-delimited, NOT multiLine)\n",
    "df_sales_bronze = (\n",
    "    spark.read \\\n",
    "    .schema(schema) \\\n",
    "    .json(\"../streaming/sales/\") \\\n",
    "    .withColumn(\"receipt_time\", current_timestamp()) \\\n",
    "    .withColumn(\"source_file\", input_file_name())\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Read {df_sales_bronze.count()} records from JSON files\")\n",
    "\n",
    "# Write to bronze as parquet\n",
    "df_sales_bronze.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(bronze_output)\n",
    "\n",
    "print(f\"âœ… Bronze layer complete!\")\n",
    "\n",
    "# Verify\n",
    "df_verify = spark.read.parquet(bronze_output)\n",
    "print(f\"âœ“ Total records in bronze: {df_verify.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9dda5f3d-7932-4733-9cd5-adf8a4ea9328",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sales_001.json: 340 records\n",
      "sales_002.json: 340 records\n",
      "sales_003.json: 340 records\n",
      "sales_004.json: 340 records\n",
      "sales_005.json: 340 records\n",
      "sales_006.json: 340 records\n",
      "sales_007.json: 340 records\n",
      "sales_008.json: 340 records\n",
      "sales_009.json: 340 records\n",
      "sales_010.json: 340 records\n",
      "\n",
      "Total records in all files: 3400\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Check how many records are in each JSON file\n",
    "total = 0\n",
    "for i in range(1, 11):\n",
    "    df = pd.read_json(f\"../streaming/sales/sales_{i:03d}.json\", lines=True)\n",
    "    print(f\"sales_{i:03d}.json: {len(df)} records\")\n",
    "    total += df.shape[0]\n",
    "    \n",
    "print(f\"\\nTotal records in all files: {total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81fbd4a-673f-45a8-81ed-2a5d5fdaa61f",
   "metadata": {},
   "source": [
    "### Silver Layer - Join with Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fd7341a-a39d-4cc5-ac99-13f9a8dfee62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Silver transformations defined\n",
      "  Records before write: 3400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/18 05:00:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Silver layer complete!\n",
      "âœ“ Total records in silver: 3400\n",
      "\n",
      "NULL foreign key counts:\n",
      "+------------+-----------+-----------+--------+\n",
      "|customer_key|product_key|payment_key|date_key|\n",
      "+------------+-----------+-----------+--------+\n",
      "|           0|          0|          0|     853|\n",
      "+------------+-----------+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define silver paths\n",
    "silver_output = \"../lakehouse/sales/silver\"\n",
    "\n",
    "# Read from bronze (batch) and join with dimensions\n",
    "df_sales_silver = spark.read.parquet(bronze_output) \\\n",
    "    .join(df_dim_customers, \n",
    "          col(\"Customer Reference ID\") == df_dim_customers.customer_reference_id, \n",
    "          \"left_outer\") \\\n",
    "    .join(df_dim_products, \n",
    "          col(\"Item Purchased\") == df_dim_products.item_name, \n",
    "          \"left_outer\") \\\n",
    "    .join(df_dim_payment, \n",
    "          col(\"Payment Method\") == df_dim_payment.payment_method, \n",
    "          \"left_outer\") \\\n",
    "    .join(df_dim_date, \n",
    "          to_date(col(\"Date Purchase\")) == df_dim_date.full_date, \n",
    "          \"left_outer\") \\\n",
    "    .select(\n",
    "        col(\"customer_key\"),\n",
    "        col(\"product_key\"),\n",
    "        col(\"payment_key\"),\n",
    "        col(\"date_key\"),\n",
    "        col(\"Purchase Amount (USD)\").alias(\"purchase_amount\"),\n",
    "        col(\"Review Rating\").alias(\"review_rating\"),\n",
    "        col(\"receipt_time\"),\n",
    "        col(\"source_file\")\n",
    "    )\n",
    "\n",
    "print(f\"âœ“ Silver transformations defined\")\n",
    "print(f\"  Records before write: {df_sales_silver.count()}\")\n",
    "\n",
    "# Write to silver as parquet\n",
    "df_sales_silver.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(silver_output)\n",
    "\n",
    "print(f\"\\nâœ… Silver layer complete!\")\n",
    "\n",
    "# Verify\n",
    "df_silver_verify = spark.read.parquet(silver_output)\n",
    "print(f\"âœ“ Total records in silver: {df_silver_verify.count()}\")\n",
    "\n",
    "# Check NULL foreign keys\n",
    "null_counts = df_silver_verify.select(\n",
    "    [count(when(col(c).isNull(), c)).alias(c) for c in ['customer_key', 'product_key', 'payment_key', 'date_key']]\n",
    ")\n",
    "print(\"\\nNULL foreign key counts:\")\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1b0b8b-195f-4f04-b9a3-c9803c1defd1",
   "metadata": {},
   "source": [
    "### Verify Silver Layer Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b78a4f8-deb8-4a77-97f6-758f58cb63fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Silver layer records: 3400\n",
      "âœ“ Columns: ['customer_key', 'product_key', 'payment_key', 'date_key', 'purchase_amount', 'review_rating', 'receipt_time', 'source_file']\n",
      "\n",
      "Sample silver records:\n",
      "+------------+-----------+-----------+--------+---------------+-------------+--------------------+--------------------+\n",
      "|customer_key|product_key|payment_key|date_key|purchase_amount|review_rating|        receipt_time|         source_file|\n",
      "+------------+-----------+-----------+--------+---------------+-------------+--------------------+--------------------+\n",
      "|         116|         33|          1|    NULL|          170.0|          3.6|2025-12-18 05:00:...|file:///Users/sim...|\n",
      "|         116|         28|          3|    NULL|           NULL|          1.2|2025-12-18 05:00:...|file:///Users/sim...|\n",
      "|         161|         23|          3|     106|          162.0|          1.1|2025-12-18 05:00:...|file:///Users/sim...|\n",
      "|         152|         41|          1|      78|         2356.0|          4.8|2025-12-18 05:00:...|file:///Users/sim...|\n",
      "|         152|         21|          1|      42|         2814.0|          4.4|2025-12-18 05:00:...|file:///Users/sim...|\n",
      "|         157|          5|          1|      20|           24.0|          1.2|2025-12-18 05:00:...|file:///Users/sim...|\n",
      "|         152|         50|          1|     253|           NULL|          2.5|2025-12-18 05:00:...|file:///Users/sim...|\n",
      "|         157|         20|          3|     178|           NULL|          1.3|2025-12-18 05:00:...|file:///Users/sim...|\n",
      "|         157|         54|          3|     140|           22.0|          1.5|2025-12-18 05:00:...|file:///Users/sim...|\n",
      "|         152|         36|          1|    NULL|           NULL|          4.5|2025-12-18 05:00:...|file:///Users/sim...|\n",
      "+------------+-----------+-----------+--------+---------------+-------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "NULL foreign key counts:\n",
      "+------------+-----------+-----------+--------+\n",
      "|customer_key|product_key|payment_key|date_key|\n",
      "+------------+-----------+-----------+--------+\n",
      "|           0|          0|          0|     853|\n",
      "+------------+-----------+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read silver data to verify\n",
    "df_silver_check = spark.read.parquet(silver_output)\n",
    "\n",
    "print(f\"âœ“ Silver layer records: {df_silver_check.count()}\")\n",
    "print(f\"âœ“ Columns: {df_silver_check.columns}\")\n",
    "print(\"\\nSample silver records:\")\n",
    "df_silver_check.show(10)\n",
    "\n",
    "# Check for NULL foreign keys\n",
    "null_counts = df_silver_check.select(\n",
    "    [count(when(col(c).isNull(), c)).alias(c) for c in ['customer_key', 'product_key', 'payment_key', 'date_key']]\n",
    ")\n",
    "print(\"\\nNULL foreign key counts:\")\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e114eeab-3b1f-407f-a48f-27a4c7323139",
   "metadata": {},
   "source": [
    "### Gold Layer 1: Category Performance by Quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58f30c3b-eb1c-40bd-aa91-25b27eff9dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/18 05:00:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Gold layer (Category) records: 31\n",
      "\n",
      "Top 10 Category Performance by Revenue:\n",
      "+-----------+-------+----+----------------+-------------+------------------+------------------+\n",
      "|category   |quarter|year|num_transactions|total_revenue|avg_transaction   |avg_rating        |\n",
      "+-----------+-------+----+----------------+-------------+------------------+------------------+\n",
      "|Accessories|2      |2023|213             |28520.0      |161.12994350282486|3.081218274111675 |\n",
      "|Accessories|1      |2023|222             |26611.0      |147.02209944751382|3.0898989898989897|\n",
      "|Tops       |1      |2023|200             |25895.0      |156.93939393939394|2.773157894736845 |\n",
      "|Tops       |2      |2023|191             |23882.0      |155.07792207792207|3.0110465116279075|\n",
      "|Accessories|3      |2023|216             |23048.0      |130.95454545454547|3.0393782383419676|\n",
      "|Bottoms    |2      |2023|141             |20429.0      |185.71818181818182|3.079069767441861 |\n",
      "|Tops       |3      |2023|194             |18531.0      |119.55483870967743|2.907303370786518 |\n",
      "|Outerwear  |2      |2023|116             |18152.0      |189.08333333333334|2.789908256880733 |\n",
      "|Footwear   |2      |2023|82              |16988.0      |261.3538461538462 |3.205405405405406 |\n",
      "|Bottoms    |3      |2023|133             |16186.0      |144.51785714285714|2.9152542372881354|\n",
      "+-----------+-------+----+----------------+-------------+------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "âœ… Gold layer (Category) complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/18 05:00:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "# Read silver (batch) and create gold aggregation\n",
    "df_sales_gold_category = spark.read.parquet(silver_output) \\\n",
    "    .join(df_dim_products, \"product_key\") \\\n",
    "    .join(df_dim_date, \"date_key\") \\\n",
    "    .groupBy(\"category\", \"quarter\", \"year\") \\\n",
    "    .agg(\n",
    "        count(\"customer_key\").alias(\"num_transactions\"),\n",
    "        sum(\"purchase_amount\").alias(\"total_revenue\"),\n",
    "        avg(\"purchase_amount\").alias(\"avg_transaction\"),\n",
    "        avg(\"review_rating\").alias(\"avg_rating\")\n",
    "    ) \\\n",
    "    .orderBy(desc(\"total_revenue\"))\n",
    "\n",
    "print(f\"âœ“ Gold layer (Category) records: {df_sales_gold_category.count()}\")\n",
    "print(\"\\nTop 10 Category Performance by Revenue:\")\n",
    "df_sales_gold_category.show(10, truncate=False)\n",
    "\n",
    "print(\"\\nâœ… Gold layer (Category) complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9a67e0-9eca-4c5b-9103-a572b2d84f7a",
   "metadata": {},
   "source": [
    "### Gold Layer 2: Customer Loyalty Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef7b169f-0441-4ea6-862c-ae190c3579b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/18 05:00:51 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:51 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:51 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:51 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:52 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Gold layer (Loyalty) records: 18\n",
      "\n",
      "Top 10 Customer Loyalty Performance:\n",
      "+--------------+-----------+-------------+-----------+------------------+----------------+\n",
      "|loyalty_member|category   |num_purchases|total_spent|avg_purchase      |unique_customers|\n",
      "+--------------+-----------+-------------+-----------+------------------+----------------+\n",
      "|Yes           |Accessories|471          |63996.0    |168.85488126649076|91              |\n",
      "|Yes           |Tops       |424          |56737.0    |164.45507246376812|93              |\n",
      "|No            |Tops       |352          |40376.0    |141.6701754385965 |76              |\n",
      "|No            |Accessories|397          |38852.0    |115.97611940298508|78              |\n",
      "|Yes           |Bottoms    |264          |33513.0    |151.64253393665157|89              |\n",
      "|Yes           |Outerwear  |238          |31071.0    |161.828125        |91              |\n",
      "|No            |Footwear   |166          |29247.0    |219.90225563909775|67              |\n",
      "|No            |Bottoms    |236          |29028.0    |156.9081081081081 |72              |\n",
      "|Yes           |Footwear   |209          |28249.0    |164.23837209302326|85              |\n",
      "|No            |Outerwear  |176          |19703.0    |138.75352112676057|70              |\n",
      "+--------------+-----------+-------------+-----------+------------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "âœ… Gold layer (Loyalty) complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/18 05:00:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "# Create second gold aggregation: Customer Loyalty Performance\n",
    "df_sales_gold_loyalty = spark.read.parquet(silver_output) \\\n",
    "    .join(df_dim_customers, \"customer_key\") \\\n",
    "    .join(df_dim_products, \"product_key\") \\\n",
    "    .groupBy(\"loyalty_member\", \"category\") \\\n",
    "    .agg(\n",
    "        count(\"customer_key\").alias(\"num_purchases\"),\n",
    "        sum(\"purchase_amount\").alias(\"total_spent\"),\n",
    "        avg(\"purchase_amount\").alias(\"avg_purchase\"),\n",
    "        approx_count_distinct(\"customer_key\").alias(\"unique_customers\")\n",
    "    ) \\\n",
    "    .orderBy(desc(\"total_spent\"))\n",
    "\n",
    "print(f\"âœ“ Gold layer (Loyalty) records: {df_sales_gold_loyalty.count()}\")\n",
    "print(\"\\nTop 10 Customer Loyalty Performance:\")\n",
    "df_sales_gold_loyalty.show(10, truncate=False)\n",
    "\n",
    "print(\"\\nâœ… Gold layer (Loyalty) complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebcf27e-daea-46f3-a1ff-5c45dfa4ae45",
   "metadata": {},
   "source": [
    "### Pipeline Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ada52c9-b739-4170-807a-b265864cb719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STREAMING ETL PIPELINE COMPLETE\n",
      "================================================================================\n",
      "âœ“ Bronze Layer: 3400 records\n",
      "âœ“ Silver Layer: 3400 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/18 05:00:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Gold Layer (Category): 31 aggregates\n",
      "âœ“ Gold Layer (Loyalty): 18 aggregates\n",
      "================================================================================\n",
      "\n",
      "âœ… Medallion Architecture Successfully Implemented!\n",
      "   - Bronze: Raw batch data with metadata (3,400 records)\n",
      "   - Silver: Cleaned and joined with dimensions (3,400 records)\n",
      "   - Gold: Business-level aggregations (31 + 18 aggregates)\n",
      "\n",
      "ðŸ“Š Key Insights:\n",
      "   - Top Category: Accessories (Q2 2023) - $28520.00 revenue\n",
      "   - Loyalty Members spend more: ~$165 avg vs ~$140 avg\n",
      "   - 853 records with NULL dates (25% - date mismatches)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/18 05:00:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 05:00:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"STREAMING ETL PIPELINE COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"âœ“ Bronze Layer: {spark.read.parquet(bronze_output).count()} records\")\n",
    "print(f\"âœ“ Silver Layer: {spark.read.parquet(silver_output).count()} records\")\n",
    "print(f\"âœ“ Gold Layer (Category): {df_sales_gold_category.count()} aggregates\")\n",
    "print(f\"âœ“ Gold Layer (Loyalty): {df_sales_gold_loyalty.count()} aggregates\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nâœ… Medallion Architecture Successfully Implemented!\")\n",
    "print(\"   - Bronze: Raw batch data with metadata (3,400 records)\")\n",
    "print(\"   - Silver: Cleaned and joined with dimensions (3,400 records)\")\n",
    "print(\"   - Gold: Business-level aggregations (31 + 18 aggregates)\")\n",
    "print(\"\\nðŸ“Š Key Insights:\")\n",
    "print(f\"   - Top Category: Accessories (Q2 2023) - ${28520:.2f} revenue\")\n",
    "print(f\"   - Loyalty Members spend more: ~$165 avg vs ~$140 avg\")\n",
    "print(f\"   - 853 records with NULL dates (25% - date mismatches)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f14e821-0255-426d-92f0-0b470a2b1dfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
